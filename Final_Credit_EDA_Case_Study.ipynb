{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit EDA Case Study**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.width',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../Credit_EDA_Case_Study/application_data.csv does not exist: '../Credit_EDA_Case_Study/application_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8c609e56f85b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading dataset from my local drive location.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Credit_EDA_Case_Study/application_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../Credit_EDA_Case_Study/application_data.csv does not exist: '../Credit_EDA_Case_Study/application_data.csv'"
     ]
    }
   ],
   "source": [
    "# Reading dataset from my local drive location.\n",
    "\n",
    "df=pd.read_csv(\"../Credit_EDA_Case_Study/application_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking structure of data(Normal Routine Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the structure of data - Normal routine check using Shape,info and describe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of missing values for all the columns\n",
    "print(round(100*(df.isnull().sum()/len(df.index)), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the null values columns having more than 50%\n",
    "blankcol=df.isnull().sum()\n",
    "blankcol=blankcol[blankcol.values>(0.5*len(df))]\n",
    "print(len(blankcol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 41 columns having  null values greater than 50% in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing those 41 columns\n",
    "blankcol = df.isnull().sum()\n",
    "blankcol=list(blankcol[blankcol.values>=0.5*len(df)].index)\n",
    "df.drop(labels=blankcol,axis=1,inplace=True)\n",
    "print(len(blankcol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the columns having less null percentage\n",
    "df.isnull().sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing columns that has less percentage(13% or so) to report best metric for imputing missing data\n",
    "df.AMT_REQ_CREDIT_BUREAU_HOUR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_DAY.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_WEEK.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_MON.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_QRT.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c2bc0f47d714>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAMT_REQ_CREDIT_BUREAU_YEAR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_YEAR.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to tell whether we use median or mean for imputing the missing values in above columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_HOUR.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_DAY.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_WEEK.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_MON.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_QRT.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AMT_REQ_CREDIT_BUREAU_YEAR.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For AMT_REQ_CREDIT_BUREAU_HOUR, AMT_REQ_CREDIT_BUREAU_DAY,AMT_REQ_CREDIT_BUREAU_WEEK,AMT_REQ_CREDIT_BUREAU_MON,AMT_REQ_CREDIT_BUREAU_YEAR\n",
    "The above chart shows that the distribution is right-skewed, and there are extreme higher values at the right of the histogram. We replace the extreme values with median values. It is advised to not use mean values as they are affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking datatype of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for nummber of unique categories in a column\n",
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change negative datatypes of below columns\n",
    "df['DAYS_BIRTH'] = df['DAYS_BIRTH'].abs()\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].abs()\n",
    "df['DAYS_REGISTRATION'] = df['DAYS_REGISTRATION'].abs()\n",
    "df['DAYS_ID_PUBLISH'] = df['DAYS_ID_PUBLISH'].abs()\n",
    "df['DAYS_LAST_PHONE_CHANGE'] = df['DAYS_LAST_PHONE_CHANGE'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset after changing negative data types\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the columns for missing %age\n",
    "df.isnull().sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove unwanted columns from this dataset and continue with 25-30 columns further\n",
    "\n",
    "unwantedcol=['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n",
    "       'FLAG_PHONE', 'FLAG_EMAIL','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL','CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n",
    "       'REGION_RATING_CLIENT_W_CITY','DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n",
    "       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n",
    "       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n",
    "       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "       'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR',\n",
    "        'YEARS_BEGINEXPLUATATION_AVG','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MODE','FLOORSMAX_MODE','YEARS_BEGINEXPLUATATION_MEDI',\n",
    "        'FLOORSMAX_MEDI','TOTALAREA_MODE','EMERGENCYSTATE_MODE','AMT_GOODS_PRICE','NAME_TYPE_SUITE','OCCUPATION_TYPE',\n",
    "        'EXT_SOURCE_2','EXT_SOURCE_3','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "        'DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "\n",
    "df.drop(labels=unwantedcol,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the columns for percentage missing values\n",
    "df.isnull().sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain columns where values are XNA which implies 'Not available'. Inspecting and updating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Gender code column\n",
    "df['CODE_GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['CODE_GENDER']=='XNA'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Female is in majority and only 4 rows are having XNA values, we can update those columns with Gender 'F' as there will be no impact on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating'CODE_GENDER' column with \"F\" for the dataset\n",
    "\n",
    "df.loc[df['CODE_GENDER']=='XNA','CODE_GENDER']='F'\n",
    "df['CODE_GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Gender code column and oberved there are 18% XNA values. Hence decided to drop these rows\n",
    "df['ORGANIZATION_TYPE'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence, dropping the rows of total 55374 have 'XNA' values in the organization type column\n",
    "\n",
    "df=df.drop(df.loc[df['ORGANIZATION_TYPE']=='XNA'].index)\n",
    "df[df['ORGANIZATION_TYPE']=='XNA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting all variable into numeric in the dataset\n",
    "\n",
    "numeric_columns=['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','REGION_POPULATION_RELATIVE','DAYS_BIRTH',\n",
    "                'DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','HOUR_APPR_PROCESS_START','LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n",
    "       'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']\n",
    "\n",
    "df[numeric_columns]=df[numeric_columns].apply(pd.to_numeric)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning of Continous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bins for these two continous variable categories column 'AMT_INCOME_TOTAL' and 'AMT_CREDIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins for AMT_INCOME_TOTAL\n",
    "\n",
    "bins = [0,25000,50000,75000,100000,125000,150000,175000,200000,225000,250000,275000,300000,325000,350000,375000,400000,425000,450000,475000,500000,10000000000]\n",
    "slot = ['0-25000', '25000-50000','50000-75000','75000,100000','100000-125000', '125000-150000', '150000-175000','175000-200000',\n",
    "       '200000-225000','225000-250000','250000-275000','275000-300000','300000-325000','325000-350000','350000-375000',\n",
    "       '375000-400000','400000-425000','425000-450000','450000-475000','475000-500000','500000 and above']\n",
    "\n",
    "df['AMT_INCOME_RANGE']=pd.cut(df['AMT_INCOME_TOTAL'],bins,labels=slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins for AMT_CREDIT\n",
    "\n",
    "bins = [0,150000,200000,250000,300000,350000,400000,450000,500000,550000,600000,650000,700000,750000,800000,850000,900000,1000000000]\n",
    "slots = ['0-150000', '150000-200000','200000-250000', '250000-300000', '300000-350000', '350000-400000','400000-450000',\n",
    "        '450000-500000','500000-550000','550000-600000','600000-650000','650000-700000','700000-750000','750000-800000',\n",
    "        '800000-850000','850000-900000','900000 and above']\n",
    "\n",
    "df['AMT_CREDIT_RANGE']=pd.cut(df['AMT_CREDIT'],bins=bins,labels=slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Bins are created in dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total no. of rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into two dataset of  target=1(clients defaulted) and target=0 (not defaulted)\n",
    "\n",
    "target0_df=df[df[\"TARGET\"]==0]\n",
    "target1_df=df[df[\"TARGET\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking no. of rows in both data sets\n",
    "print(target0_df.shape)\n",
    "\n",
    "print(target1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalance Percentage\n",
    "# percentage of values in taget0_df and target1_df\n",
    "\n",
    "print(round(100*(len(target0_df.index)/252137),2))\n",
    "print(round(100*(len(target1_df.index)/252137),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis for categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Univariate Analysis in logarithmic scale for target=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plotting in logarithmic scale\n",
    "\n",
    "def uniplot(df,col,title,hue =None):\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set_context('talk')\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams['axes.titlesize'] = 22\n",
    "    plt.rcParams['axes.titlepad'] = 30\n",
    "    \n",
    "    \n",
    "    temp = pd.Series(data = hue)\n",
    "    fig, ax = plt.subplots()\n",
    "    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n",
    "    fig.set_size_inches(width , 8)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='viridis') \n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLotting for income range\n",
    "\n",
    "uniplot(target0_df,col='AMT_INCOME_RANGE',title='Income range Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Female count is higher than male.\n",
    "2. Income range from 100000 to 200000 is having more number of credits.\n",
    "3. It shows that Females are more than male in having credits for that range.\n",
    "4. Count is very less for income range 400000 and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Contract type\n",
    "\n",
    "uniplot(target0_df,col='NAME_CONTRACT_TYPE',title='Contract type Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. For ‘cash loans’ type contract credits are higher than ‘Revolving loans’ contract type.\n",
    "2. Females are leading for applying credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Income type\n",
    "\n",
    "uniplot(target0_df,col='NAME_INCOME_TYPE',title='Income type Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Females are having more number of credits than male.\n",
    "2. Less number of credits for income type ‘student’ ,’pensioner’, ‘Businessman’ and ‘Maternity leave’.\n",
    "3. For income type ‘working’, ’commercial associate’, and ‘State Servant’ the number of credits are higher than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Organization type in logarithmic scale\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.titlepad'] = 30\n",
    "\n",
    "plt.title(\"Organization type distribution for target - 0\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xscale('log')\n",
    "\n",
    "sns.countplot(data=target0_df,y='ORGANIZATION_TYPE',order=target0_df['ORGANIZATION_TYPE'].value_counts().index,palette='cividis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Clients which have applied for credits are from most of the organization type ‘Business entity Type 3’ , ‘Self employed’,\n",
    "   ‘Other’ , ‘Medicine’ and ‘Government’.\n",
    "2. Less clients are from Industry type 10,type 8, type 6, religion and  trade type 4, type 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categoroical Univariate Analysis in logarithmic scale for target=1(clients defaulted)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLotting for income range\n",
    "\n",
    "uniplot(target1_df,col='AMT_INCOME_RANGE',title='Income range Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Less count for income range 400000 and above.\n",
    "2. Male counts are higher than female.\n",
    "3. Income range from 100000 to 200000 is having more number of credits.\n",
    "4. This shows that Males are more than female in having credits for that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Contract type\n",
    "\n",
    "uniplot(target1_df,col='NAME_CONTRACT_TYPE',title='Contract type Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. ‘Cash loans’ contract type is having higher number of credits than ‘Revolving loans’ contract type.\n",
    "2. Female is leading for applying credits.\n",
    "3. There are only Female Revolving loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Income type\n",
    "\n",
    "uniplot(target1_df,col='NAME_INCOME_TYPE',title='Income type Distribution',hue='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Less number of credits for income type  ‘Maternity leave’.\n",
    "2. For income type ‘working’, ’commercial associate’, and ‘State Servant’ the number of credits are higher.\n",
    "3. Females are having more number of credits than male.\n",
    "4. There is no income type for ‘student’ , ’pensioner’ and ‘Businessman’ which means they don’t do any late payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for Organization type in logarithmic scale\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.titlepad'] = 30\n",
    "\n",
    "plt.title(\"Organization type distribution for target - 1\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xscale('log')\n",
    "\n",
    "sns.countplot(data=target0_df,y='ORGANIZATION_TYPE',order=target1_df['ORGANIZATION_TYPE'].value_counts().index,palette='cividis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from the above graph.\n",
    "\n",
    "1. Clients which have applied for credits are from most of the organization type ‘Business entity Type 3’ , ‘Self employed’ , ‘Other’ , ‘Medicine’ and ‘Government’.\n",
    "2. There are less clients from Industry type 8,type 6, type 13, type 10, religion and  trade type 4, type 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding correlation for numerical columns for both cases - Target 0 and Target1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target0_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4bd658ecfcac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Finding correlation for numerical columns for target 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget0_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target0_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Finding correlation for numerical columns for target 0\n",
    "\n",
    "corr = target0_df.corr()\n",
    "\n",
    "corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "corr0_df = corr.unstack().reset_index()\n",
    "corr0_df.columns = ['VAR1', 'VAR2', 'Correlation']\n",
    "corr0_df.dropna(subset= ['Correlation'], inplace= True)\n",
    "corr0_df['Correlation'] = round(corr0_df['Correlation'],2)\n",
    "# Since we see correlation as absolute value, we are converting it into absolute value\n",
    "corr0_df['Correlation'] = corr0_df['Correlation'].abs()\n",
    "corr0_df.sort_values(by = 'Correlation', ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding correlation for numerical columns for target 1\n",
    "\n",
    "corr = target1_df.corr()\n",
    "\n",
    "corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "corr1_df = corr.unstack().reset_index()\n",
    "corr1_df.columns = ['VAR1', 'VAR2', 'Correlation']\n",
    "corr1_df.dropna(subset= ['Correlation'], inplace= True)\n",
    "corr1_df['Correlation'] = round(corr0_df['Correlation'],2)\n",
    "# Since we see correlation as absolute value, we are converting it into absolute value\n",
    "corr1_df['Correlation'] = corr0_df['Correlation'].abs()\n",
    "corr1_df.sort_values(by = 'Correlation', ascending= False).head (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 variables from Target 0 and Target 1 can be found above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Variables with Highest correlation are same in both the files - Target 0 and Target 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for univariate variables analysis in logarithmic scale\n",
    "\n",
    "def univariate_numerical(data,col,title):\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set_context('talk')\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams['axes.titlesize'] = 22\n",
    "    plt.rcParams['axes.titlepad'] = 30\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.yscale('log')\n",
    "    sns.boxplot(data =df, x=col,orient='v')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Target 0 - Finding any outliers** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of income amount\n",
    "\n",
    "univariate_numerical(data=target0_df,col='AMT_INCOME_TOTAL',title='Distribution of income amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Some outliers are noticed in income amount.\n",
    "2. The third quartiles is very slim for income amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disrtibution of credit amount\n",
    "\n",
    "univariate_numerical(data=target0_df,col='AMT_CREDIT',title='Distribution of credit amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Some outliers are noticed in credit amount.\n",
    "2. The third quartiles is relative slim for credit amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of anuuity amount\n",
    "\n",
    "univariate_numerical(data=target0_df,col='AMT_ANNUITY',title='Distribution of Annuity amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Some outliers are noticed in annuity amount as well.\n",
    "2. The third and second quartiles are almost same for annuity amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Target 1 - Finding any outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of income amount\n",
    "\n",
    "univariate_numerical(data=target1_df,col='AMT_INCOME_TOTAL',title='Distribution of income amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Some outliers are noticed in distribution of income amount.\n",
    "2. The third and second quartiles are almost same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of credit amount\n",
    "\n",
    "univariate_numerical(data=target1_df,col='AMT_CREDIT',title='Distribution of credit amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Not much outliers are noticed in distribution of credit amount.\n",
    "2. The third quartile is relatively smaller than all the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Annuity amount\n",
    "\n",
    "univariate_numerical(data=target1_df,col='AMT_ANNUITY',title='Distribution of Annuity amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few points concluded from the graph above.\n",
    "\n",
    "1. Few outliers are noticed in distribution of annuity.\n",
    "2. The third and second quartiles are almost same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate analysis for numerical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate analysis for Target 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for Income amount\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=45)\n",
    "plt.yscale('log')\n",
    "sns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\n",
    "plt.title('Income Amount vs Education Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points from above box plot-\n",
    "\n",
    "1. For Education type 'Higher education' the income amount is mostly equal with family status but has many outliers. \n",
    "2. Less outlier are having for Academic degree but there income amount is little high that Higher education.\n",
    "3. Lower secondary of civil marriage family status are have less income amount than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for Credit amount\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=45)\n",
    "sns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\n",
    "plt.title('Credit Amount vs Education Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points from above box plot:\n",
    "\n",
    "1. Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others.\n",
    "2. Civil marriage for Academic degree is having most of the credits in the third quartile.\n",
    "3. Higher education of family status of 'marriage', 'single' and 'civil marriage' are having more outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate analysis for Target 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for Income amount\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=45)\n",
    "plt.yscale('log')\n",
    "sns.boxplot(data =target1_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\n",
    "plt.title('Income Amount vs Education Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points from above box plot:\n",
    "\n",
    "1. For Education type 'Lower education' the income amount very different with family status and has significant outliers. \n",
    "2. Lower secondary of widow family status have less income amount than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for credit amount\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=45)\n",
    "sns.boxplot(data =target1_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\n",
    "plt.title('Credit Amount vs Education Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points from above box plot:\n",
    "\n",
    "1. Family status of 'widow' and 'married' of Secondary Special degree education are having higher number of credits than others.\n",
    "2. Higher education of family status of 'single', and 'married' are having more outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading previous application data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading \"previous application\" data\n",
    "\n",
    "df1=pd.read_csv(\"../Credit_EDA_Case_Study/previous_application.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the structure of data\n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of missing values for all the columns\n",
    "print(round(100*(df1.isnull().sum()/len(df1.index)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the null values columns having more than 40%\n",
    "blankcol1=df1.isnull().sum()\n",
    "blankcol1=blankcol1[blankcol1.values>(0.4*len(df1))]\n",
    "print(len(blankcol1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 columns having  null values greater than 40% in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing those 11 columns\n",
    "blankcol1 = df1.isnull().sum()\n",
    "blankcol1=list(blankcol1[blankcol1.values>=0.4*len(df1)].index)\n",
    "df1.drop(labels=blankcol1,axis=1,inplace=True)\n",
    "print(len(blankcol1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing values XAP, XNA in dataset\n",
    "\n",
    "df1=df1.drop(df1[df1['NAME_CASH_LOAN_PURPOSE']=='XNA'].index)\n",
    "df1=df1.drop(df1[df1['NAME_CASH_LOAN_PURPOSE']=='XAP'].index)\n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Application dataset with previous appliaction dataset\n",
    "\n",
    "final_df=pd.merge(left=df,right=df1,how='inner',on='SK_ID_CURR',suffixes='_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting dataframe\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names after merging\n",
    "\n",
    "final_df1 = final_df.rename({'NAME_CONTRACT_TYPE_' : 'NAME_CONTRACT_TYPE','AMT_CREDIT_':'AMT_CREDIT','AMT_ANNUITY_':'AMT_ANNUITY',\n",
    "                         'WEEKDAY_APPR_PROCESS_START_' : 'WEEKDAY_APPR_PROCESS_START',\n",
    "                         'HOUR_APPR_PROCESS_START_':'HOUR_APPR_PROCESS_START','NAME_CONTRACT_TYPEx':'NAME_CONTRACT_TYPE_PREV',\n",
    "                         'AMT_CREDITx':'AMT_CREDIT_PREV','AMT_ANNUITYx':'AMT_ANNUITY_PREV',\n",
    "                         'WEEKDAY_APPR_PROCESS_STARTx':'WEEKDAY_APPR_PROCESS_START_PREV',\n",
    "                         'HOUR_APPR_PROCESS_STARTx':'HOUR_APPR_PROCESS_START_PREV'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted columns for analysis\n",
    "\n",
    "final_df1.drop(['SK_ID_CURR','WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION', \n",
    "              'REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n",
    "              'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY','WEEKDAY_APPR_PROCESS_START_PREV',\n",
    "              'HOUR_APPR_PROCESS_START_PREV', 'FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY',\n",
    "               'NAME_SELLER_INDUSTRY','SELLERPLACE_AREA','DAYS_DECISION','CODE_REJECT_REASON','NAME_GOODS_CATEGORY'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing univariate analysis to get insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Loan purposes with Target\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.titlepad'] = 30\n",
    "plt.xticks(rotation=90)\n",
    "plt.xscale('log')\n",
    "plt.title('Distribution of Loan purposes with Target ')\n",
    "ax = sns.countplot(data = final_df1, y= 'NAME_CASH_LOAN_PURPOSE', \n",
    "                   order=final_df1['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'TARGET',palette='viridis') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points we can conclude from above plot:\n",
    "\n",
    "1. Loan purposes with 'Repairs' are facing more difficulites in payment on time ie, they defaulted mostly.\n",
    "2. There are few places where clients/people have done loan payment significantly well as compared to being defaulted.\n",
    "   They are 'Buying a garage', 'Business developemt', 'Buying land','Buying a new car' and 'Education'\n",
    "\n",
    "Hence we can focus on these Loan purposes for which the client/people having minimal payment difficulties ie. less defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of contract status with Purposes\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "plt.figure(figsize=(15,30))\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.titlepad'] = 30\n",
    "plt.xticks(rotation=90)\n",
    "plt.xscale('log')\n",
    "plt.title('Distribution of contract status with Loan purposes')\n",
    "ax = sns.countplot(data = final_df1, y= 'NAME_CASH_LOAN_PURPOSE', \n",
    "                   order=final_df1['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'NAME_CONTRACT_STATUS',palette='viridis') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points we can conclude from above plot:\n",
    "\n",
    "1. Most rejection of loans came from Loan purpose 'repairs'.\n",
    "2. Paying other loans and buying a new car is having significant higher rejection than approves.\n",
    "3. For education purposes we have equal number of approves and rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing Bivariate analysis to get insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for Prev Credit amount vs Housing type\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(data =final_df1, y='AMT_CREDIT_PREV',hue='TARGET',x='NAME_HOUSING_TYPE')\n",
    "plt.title('Prev Credit amount vs Housing type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points we can conclude from above plot:\n",
    "\n",
    "1. For Housing type, office appartment is having higher credit of target 0.\n",
    "2. co-op apartment is having higher credit of target 1.\n",
    "\n",
    "So, we can say that bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment ie, most of them defaulted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plotting for Prev Credit amount vs Loan purpose\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.xticks(rotation=90)\n",
    "plt.yscale('log')\n",
    "sns.boxplot(data =final_df1, x='NAME_CASH_LOAN_PURPOSE',hue='NAME_INCOME_TYPE',y='AMT_CREDIT_PREV',orient='v')\n",
    "plt.title('Prev Credit amount vs Loan Purpose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points we can conclude from above plot-\n",
    "\n",
    "1. The credit amount of Loan purposes like 'Buying a home','Buying a holiday home/land','Buying a new car' and'Building a home' is higher.\n",
    "2. Money for third person or a Hobby is having less credits applied for.\n",
    "3. Income type of state servants have a significant amount of credit applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "   1. Bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment ie, most        of them defaulted.\n",
    "   2. Banks should focus less on income type ‘Working’ as they are having most number of unsuccessful payments.\n",
    "   3. Bank should focus less on loan purpose ‘Repair’ as it is having higher number of unsuccessful payments on time.\n",
    "   4. Also, Paying other loans and buying a new car is having significant higher rejection than approves.\n",
    "   5. Get as much as clients from housing type ‘With parents’ as they are having least number of unsuccessful payments.\n",
    "   6. Banks should focus more on contract type ‘Student’ ,’pensioner’with housing ‘type other than ‘Co-op apartment’ for         successful payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
